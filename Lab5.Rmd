---
title: "Lab5"
author: "Linus Ghanadan"
date: "2023-02-07"
output: html_document
---

This week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models.

In order to use the Spotify API you must have a Spotify account. If you don't have one, sign up for a free one here: <https://www.spotify.com/us/signup>

Once you have an account, go to Spotify for developers (<https://developer.spotify.com/>) and log in. Click the green "Create a Client ID" button to fill out the form to create an app create an app so you can access the API.

On your developer dashboard page, click on the new app you just created. Go to Settings -\> Basic Information and you will find your Client ID . Click "View client secret" to access your secondary Client ID. Scroll down to Redirect URIs and enter: <http://localhost:1410/>

You have two options for completing this lab.

**Option 1**: **Classify by users**. Build models that predict whether a given song will be in your collection vs. a partner in class. This requires that you were already a Spotify user so you have enough data to work with. You will download your data from the Spotify API and then exchange with another member of class.

**Option 2**: **Classify by genres**. Build models that predict which genre a song belongs to. This will use a pre-existing Spotify dataset available from Kaggle.com (<https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify>)


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```


```{r}
# library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)
library(here)
library(patchwork)
library(baguette)
```

```{r}
# Sourcing API variable
setwd(here::here())
# api <- source('keys/keys.R')
```


Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time.

```{r access_API}

# Sys.setenv(SPOTIFY_CLIENT_ID = '78d73a6f06864c0490376e08f8dc5b50')
# 
# Sys.setenv(SPOTIFY_CLIENT_SECRET = 'api')
# 
# authorization_code <- get_spotify_authorization_code(scope = scopes()[c(1:19)]) #sets an authorization code that you'll need to provide for certain get_ functions via my_tracks <- get_my_saved_tracks(authorization = authorization_code)
# 
# access_token <- get_spotify_access_token() #takes ID and SECRET, sends to Spotify and receives an access token
```

**Option 1: Data Preparation**

You can use get_my_saved_tracks() to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with. If you don't have enough liked tracks, you can instead use get_my_recently_played(), and in that case grab at least 500 recently played tracks if you can.

```{r, eval=FALSE}
## get 50 saved tracks
my_tracks <- get_my_saved_tracks(authorization = authorization_code, limit = 50)
```

```{r, eval=FALSE}
## get additional 150 saved tracks

# set offsets for for loop
offsets = seq(from = 0, to = 150, by = 50)

# initialize an empty df 
my_tracks <- data.frame(matrix(nrow = 0, ncol = 30))

# function to get my 150 most recently liked tracks 
for (i in seq_along(offsets)) {  
  liked_tracks = get_my_saved_tracks(authorization = authorization_code, limit = 50, 
                                     offset = offsets[i])
  df_temp = as.data.frame(liked_tracks) # create temporary data frame to store the 50 liked tracks from one iteration
  my_tracks <- rbind(my_tracks, df_temp) # bind temporary data frame to my liked tracks data frame 
}
```


These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.


```{r, eval=FALSE}
# get track audio features and bind into one df
first100 <- get_track_audio_features(my_tracks$track.id[1:100])
second100 <- get_track_audio_features(my_tracks$track.id[101:200])
audio_features <- rbind(first100, second100)

# create finalized df of my liked songs with tracks, audio features, and my name
linus_tracks <- my_tracks %>% 
  select(track.name) %>% 
  cbind(audio_features) %>% 
  mutate(name = "linus")

# write CSV file to share with partner
# write.csv(linus_tracks,'linus_tracks.csv', row.names = FALSE)
```


Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.

```{r}
# read in my data (CSV that was previously written)
linus_tracks <- read.csv(here::here("lab", "data", "linus_tracks.csv"))

# read in partner data
maxwell_tracks <- read.csv(here::here("lab", "data", "maxwell_songs.csv")) %>% 
  mutate(name = "maxwell")

# bind my liked songs df with partner df
combined_tracks <- rbind(linus_tracks, maxwell_tracks)
```


###Data Exploration (both options)

Let's take a look at your data. Do some exploratory summary stats and visualization.

```{r}
# compare mean energy, instrumentalness, tempo, duration, mode, and valence
combined_tracks %>%
  group_by(name) %>%
  summarise(mean_energy = mean(energy),
            mean_instrumentalness = mean(instrumentalness),
            mean_tempo = mean(tempo),
            mean_duration = mean(duration_ms),
            mean_mode = mean(mode),
            mean_valence = mean(valence)) %>% 
  ungroup()
```


```{r}
# energy histograms
Hmisc::histbackback(split(combined_tracks$energy, combined_tracks$name),
             main = "Spotify liked songs comparison of energy", 
             ylab = "energy",
             xlab = c("linus", "maxwell"))
```

```{r}
# instrumentalness histograms
Hmisc::histbackback(split(combined_tracks$instrumentalness, combined_tracks$name),
             main = "Spotify liked songs comparison of instrumentalness", 
             ylab = "instrumentalness",
             xlab = c("linus", "maxwell"))
```

```{r}
# tempo histograms
Hmisc::histbackback(split(combined_tracks$tempo, combined_tracks$name),
             main = "Spotify liked songs comparison of tempo", 
             ylab = "tempo",
             xlab = c("linus", "maxwell"))
```

```{r}
# duration histograms
Hmisc::histbackback(split(combined_tracks$duration_ms, combined_tracks$name),
             main = "Spotify liked songs comparison of duration", 
             ylab = "duration (milliseconds)",
             xlab = c("linus", "maxwell"))
```

```{r}
# mode histograms
Hmisc::histbackback(split(combined_tracks$mode, combined_tracks$name),
             main = "Spotify liked songs comparison of mode", 
             ylab = "mode",
             xlab = c("linus", "maxwell"))
```


```{r}
# valence histograms
Hmisc::histbackback(split(combined_tracks$valence, combined_tracks$name),
             main = "Spotify liked songs comparison of valence", 
             ylab = "valence",
             xlab = c("linus", "maxwell"))
```


### **Modeling**

Create competing models that predict whether a track belongs to you or your partner's collection

You will eventually create four final candidate models:

1.  k-nearest neighbor (Week 5)

## preprocessing

```{r}
# remove irrelevant columns from combined_tracks df
combined_tracks <- combined_tracks %>% 
  select(-track.name, -type, -id, -uri, -track_href, -analysis_url)
```


```{r}
# set seed for reproducibility
set.seed(123)

# initial split of data (default 75/25)
tracks_split <- initial_split(combined_tracks)
tracks_test <- testing(tracks_split)
tracks_train <- training(tracks_split)
```

```{r}
# specify recipe for model preprocessing
tracks_recipe <- recipe(name ~ ., data = tracks_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() # prep recipe

# bake training data using recipe
baked_train <- bake(tracks_recipe, tracks_train)
```

```{r}
# create 10 folds of the training dataset for CV
cv_folds <- tracks_train %>% vfold_cv(v = 10)
```

## KNN

```{r}
# specify KNN model
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

```{r}
# create workflow
knn_workflow <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(tracks_recipe)
```

```{r}
# fit the workflow on our predefined folds and a grid of hyperparameters
fit_knn_cv <- 
  knn_workflow %>%
  tune_grid(cv_folds)
```

```{r}
# visualize how error metrics change based on k during CV
autoplot(fit_knn_cv)
```

```{r}
# finalize workflow for training data
final_wf <- knn_workflow %>%
  finalize_workflow(select_best(fit_knn_cv, metric= "accuracy")) # select best value of k based on accuracy in CV

# fit finalized workflow to training data
final_fit <- final_wf %>%
  fit(data = tracks_train)
```

```{r}
# predict testing data
test_predict_knn <- predict(final_fit, tracks_test) %>% 
  bind_cols(tracks_test) %>%  # bind to testing column
  mutate(name = as.factor(name))

# get prediction probabilities for test 
test_predict_knn <- predict(final_fit, tracks_test, type = "prob") %>%
  bind_cols(test_predict_knn) %>%  # bind to testing column
  mutate(name = as.factor(name))

```

```{r}
# visualize confusion matrix for predictions made on testing data
conf_matrix_knn <- test_predict_knn %>% 
  conf_mat(truth = name, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") +
  ggtitle("KNN") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position = "none")
```

```{r}
# store error metrics of testing data predictions
accuracy_knn <- accuracy(test_predict_knn, truth = name, estimate = .pred_class)
roc_auc_knn <- roc_auc(test_predict_knn, truth = name, .pred_linus)
sensitivity_knn <- sensitivity(test_predict_knn, truth = name, estimate = .pred_class)
specificity_knn <- specificity(test_predict_knn, truth = name, estimate = .pred_class)
```


2.  decision tree (Week 5)


```{r}
# set up decision tree specification to tell the model that we are tuning hyperparamaters
tree_spec_tune <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

```{r}
# create grid of hyperparameters
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)
```


```{r}
# create workflow with recipe and model
wf_tree_tune <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(tree_spec_tune)
```


```{r}
# tune hyperparameters
tree_rs <- tune_grid(tree_spec_tune, 
                     as.factor(name) ~ ., 
                     resamples = cv_folds,
                     grid = tree_grid,
                     metrics = metric_set(accuracy)
)
```

```{r}
# plot how error metrics change based on hyperparameters
autoplot(tree_rs) + theme_light()
```

```{r}
# finalize model specification with optimized hyperparameters
final_tree <- finalize_model(tree_spec_tune, select_best(tree_rs))

# fit finalized model to training data
final_tree_fit <- fit(final_tree, as.factor(name)~., tracks_train)
```


```{r}
# predict testing data
test_predict_tree <- predict(final_tree_fit, tracks_test) %>%
  bind_cols(tracks_test) %>%  # bind to testing column
  mutate(name = as.factor(name))

# get prediction probabilities for test 
test_predict_tree <- predict(final_tree_fit, tracks_test, type = "prob") %>%
  bind_cols(test_predict_tree) %>%  # bind to testing column
  mutate(name = as.factor(name))

```


```{r}
# create confusion matrix for predictions made on testing data
conf_matrix_tree <- test_predict_tree %>% 
  conf_mat(truth = name, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") +
  ggtitle("DT") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position = "none")
```

```{r}
# store error metrics of testing data predictions
accuracy_tree <- accuracy(test_predict_tree, truth = name, estimate = .pred_class)
roc_auc_tree <- roc_auc(test_predict_tree, truth = name, .pred_linus)
sensitivity_tree <- sensitivity(test_predict_tree, truth = name, estimate = .pred_class)
specificity_tree <- specificity(test_predict_tree, truth = name, estimate = .pred_class)
```


3.  bagged tree (Week 6)
    -   bag_tree()
    -   Use the "times =" argument when setting the engine during model specification to specify the number of trees. The rule of thumb is that 50-500 trees is usually sufficient. The bottom of that range should be sufficient here.
    
```{r}
# specify bagged tree model
bag_tree_spec <- bag_tree() %>%
  set_engine("rpart", times = 50) %>%
  set_mode("classification")
```

```{r}
# create workflow
wf_bag_tree <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(bag_tree_spec)
```

```{r}
# fit model on the training data
fit_bag_tree <- wf_bag_tree %>%
  fit(data = tracks_train)
```

```{r}
# predict testing data
test_predict_bag <- predict(fit_bag_tree, tracks_test) %>% 
  bind_cols(tracks_test) %>%  # bind to testing column
  mutate(name = as.factor(name))

# get prediction probabilities for test
test_predict_bag <- predict(fit_bag_tree, tracks_test, type = "prob") %>%
  bind_cols(test_predict_bag) %>%  # bind to testing column
  mutate(name = as.factor(name))

```

```{r}
# create confusion matrix for predictions made on testing data
conf_matrix_bag <- test_predict_bag %>%
  conf_mat(truth = name, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("Bagged DTs") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position = "none")
```

```{r}
# store error metrics of testing data predictions
accuracy_bag <- accuracy(test_predict_bag, truth = name, estimate = .pred_class)
roc_auc_bag <- roc_auc(test_predict_bag, truth = name, .pred_linus)
sensitivity_bag <- sensitivity(test_predict_bag, truth = name, estimate = .pred_class)
specificity_bag <- specificity(test_predict_bag, truth = name, estimate = .pred_class)
```

    
4.  random forest (Week 6)
    -   rand_forest()
    -   m_try() is the new hyperparameter of interest for this type of model. Make sure to include it in your tuning process
    
```{r}
# specify RF model
rf_spec <- rand_forest(trees = 500, mtry = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```

```{r}
# create a tuning grid
rf_grid <- grid_latin_hypercube(
  mtry(range = c(2, 4)), 
  min_n(c(1, 10)),
  size = 20
)
```

    
```{r}
# create workflow
wf_rf <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(rf_spec)
```


```{r}
# tune the model
rf_tune_res <- tune_grid(
  wf_rf,
  resamples = cv_folds,
  grid = rf_grid
)
```

```{r}
# visualize tuning results
autoplot(rf_tune_res)
```

```{r}
# finalize the model
best_rf_params <- select_best(rf_tune_res, "accuracy")
final_rf <- finalize_model(rf_spec, best_rf_params)
```

```{r}
# finalize the model with optimal parameters
final_rf <- finalize_model(rf_spec, best_rf_params)

# create a new workflow with finalized model
final_rf_workflow <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(final_rf)

# fit final workflow to the training data
final_rf_fit <- final_rf_workflow %>%
  fit(data = tracks_train)

```


```{r}
# predict testing data
test_predict_rf <- predict(final_rf_fit, tracks_test) %>%
  bind_cols(tracks_test) %>%
  mutate(name = as.factor(name))

 # get prediction probabilities for test
test_predict_rf <- predict(final_rf_fit, tracks_test, type = "prob") %>%
  bind_cols(test_predict_rf) %>%
  mutate(name = as.factor(name))
```


```{r}
# make confusion matrix for predictions made on testing data
conf_matrix_rf <- test_predict_rf %>%
  conf_mat(truth = name, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("RF") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position = "none")

```

```{r}
# store error metrics of testing data predictions
accuracy_rf <- accuracy(test_predict_rf, truth = name, estimate = .pred_class)
roc_auc_rf <- roc_auc(test_predict_rf, truth = name, .pred_linus)
sensitivity_rf <- sensitivity(test_predict_rf, truth = name, estimate = .pred_class)
specificity_rf <- specificity(test_predict_rf, truth = name, estimate = .pred_class)
```


Go through the modeling process for each model:

Preprocessing. You can use the same recipe for all the models you create.

Resampling. Make sure to use appropriate resampling to select the best version created by each algorithm.

Tuning. Find the best values for each hyperparameter (within a reasonable range).

Compare the performance of the four final models you have created.

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison. Use at least one visualization illustrating your model results.

```{r}
# create tibble of accuracy and ROC AUC for all four models
metrics_tibble <- tibble(
  Method = rep(c("KNN", "Decision Tree", "Bagged Trees", "Random Forest"), times = 2),
  Metric = rep(c("Accuracy", "ROC AUC"), each = 4),
  Value = c(accuracy_knn$.estimate[1], accuracy_tree$.estimate[1],
            accuracy_bag$.estimate[1], accuracy_rf$.estimate[1],
            roc_auc_knn$.estimate[1], roc_auc_tree$.estimate[1],
            roc_auc_bag$.estimate[1], roc_auc_rf$.estimate[1]))

```

```{r}
# create bar plot comparing accuracy and ROC AUC across all four models
ggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + 
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_text(aes(label = sprintf("%.2f", Value),
                y = Value + 0.02),
            position = position_dodge(width = 0.9),
            vjust = 0,
            size = 2) +
  theme_minimal() +
  labs(y = "Metric Value", x = "Model", title = "Model Comparison") +
  scale_fill_brewer(palette = "Pastel1") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# display confusion matrices of all four models
conf_matrix_knn + conf_matrix_tree + conf_matrix_bag + conf_matrix_rf +
  plot_layout(nrow = 2, ncol = 2)
```

[For predicting whether a song was liked by Maxwell or me, the Random Forest model was the most accurate (82%) and had the largest area under its Receiver Operating Characteristic (ROC) curve (0.90). The KNN and bagged tree models performed the next best, both with an accuracy of 78% and similar areas under their ROC curve (0.86 for KNN and 0.85 for bagged trees). Interestingly, when it came to predicting songs in Maxwell's collection, the KNN model performed the best (79% accuracy) of all four models, as the best overall model, Random Forest, only predicted Maxwell's collection with 76% accuracy. Lastly, our decision tree model performed the worst of all four models, with an accuracy of 74% and area under its ROC curve of 0.76 (both metrics are the lowest of all models).]{style="color:navy;"}

