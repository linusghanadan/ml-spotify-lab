---
title: "Lab6"
author: "Linus Ghanadan"
date: "2023-03-01"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```


```{r}
# load packages
library(here)
library(tidyverse)
library(tidymodels)
library(vip)
```

```{r}
# set seed for reproducibility
set.seed(123)
```

## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r}
# read in data
eel_data <- read.csv(here::here("lab", "data", "eel.model.data.csv")) %>%
  janitor::clean_names() %>% 
  mutate(angaus = as.factor(angaus),
         ds_dam = as.factor(ds_dam)) %>% 
  select(-site)
```


### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.


```{r}
# initial split of data (default 70/30)
eel_split <- initial_split(eel_data, strata = angaus, prop = 0.7)
eel_test <- testing(eel_split)
eel_train <- training(eel_split)
```

```{r}
# create 10 folds of the training dataset for CV
cv_folds <- eel_train %>% vfold_cv(v = 10)
```



### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r}
# specify recipe for model preprocessing
eel_recipe <- recipe(angaus ~ ., data = eel_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() # prep recipe

# bake training data using recipe
baked_train <- bake(eel_recipe, eel_train)
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
# specify model for tuning learning rate
xgboost_spec <- boost_tree(mode = "classification",
                           engine = "xgboost",
                           learn_rate = tune())
                            
```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
# create tuning grid for learning rate
lr_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
# create workflow for tuning learning rate
xgboost_workflow <- workflow() %>%
  add_model(xgboost_spec) %>%
  add_recipe(eel_recipe)
```

```{r}
# tune the model for optimal learning rate
xgboost_tune <- tune_grid(
  xgboost_workflow,
  resamples = cv_folds,
  grid = lr_grid
)
```


```{r}
# store optimized learning rate
best_lr <- select_best(xgboost_tune, "accuracy")
best_lr$learn_rate
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
# specify model with optimized learning rate to tune tree-based parameters
xgboost_spec_tree <- boost_tree(
  trees = 3000,
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = best_lr$learn_rate,
  loss_reduction = tune(),
  mode = "classification",
  engine = "xgboost"
)
```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
# create tuning grid for tree-based parameters
tree_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  size = 10)


```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
# create workflow for tuning tree-based parameters
xgboost_workflow_tree <- workflow() %>%
  add_model(xgboost_spec_tree) %>%
  add_recipe(eel_recipe)

# tune tree-based parameters
xgboost_tree_tune <- tune_grid(
  xgboost_workflow_tree,
  resamples = cv_folds,
  grid = tree_grid
)
```

```{r}
# collect metrics for best models
best_models_metrics <- xgboost_tree_tune %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>% # assuming accuracy is your metric of interest
  arrange(desc(mean)) # arrange by the highest mean accuracy

# store parameters of best model
best_tree_params <- select_best(xgboost_tree_tune, "accuracy")

# show metrics of best models
print(best_models_metrics)

```



### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
# specify model with optimized learning rate and tree parameters to tune based on stochastic parameters
xgboost_spec_stochastic <- boost_tree(
  trees = 3000,
  tree_depth = best_tree_params$tree_depth,
  min_n = best_tree_params$min_n,
  learn_rate = best_lr$learn_rate,
  loss_reduction = best_tree_params$loss_reduction,
  mtry = tune(),
  sample_size = tune(),
  mode = "classification",
  engine = "xgboost"
)
```



2.  Set up a tuning grid. Use grid_latin_hypercube() again.

```{r}
# finalize mtry range based on the number of predictors
mtry_finalized <- finalize(mtry(), baked_train)

# create tuning grid for stochastic parameters
stochastic_grid <- grid_latin_hypercube(
  mtry_finalized,
  sample_size = sample_prop(),
  size = 10
)

```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
# create workflow for tuning stochastic parameters
xgboost_workflow_stochastic <- workflow() %>%
  add_model(xgboost_spec_stochastic) %>%
  add_recipe(eel_recipe)

# tune stochastic parameters
xgboost_stochastic_tune <- tune_grid(
  xgboost_workflow_stochastic,
  resamples = cv_folds,
  grid = stochastic_grid
)
```

```{r}
# collect metrics for best models
best_stochastic_metrics <- xgboost_stochastic_tune %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean)) # arrange by highest mean accuracy

# store parameters of best model
best_stochastic_params <- select_best(xgboost_stochastic_tune, "accuracy")

# show metrics of best models
print(best_stochastic_metrics)

```



## Finalize workflow and make final prediction

```{r}
# specify final model with optimized parameters
final_xgboost_model <- finalize_model(
  xgboost_spec_stochastic,
  best_stochastic_params
)

# fit final model to training data
final_fit <- fit(
  final_xgboost_model,
  formula = angaus ~ .,
  data = baked_train
)
```

```{r}
# bake training data using recipe
baked_test <- bake(eel_recipe, eel_test)

# predict testing data
test_predict <- predict(final_fit, baked_test) %>%
  bind_cols(baked_test) # bind predictions column to testing data

# get prediction probabilities
test_predict <- predict(final_fit, baked_test, type = "prob") %>%
  bind_cols(test_predict) # bind predictions column to class predictions data frame

# create confusion matrix
conf_mat(data = test_predict, truth = angaus, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```


1.  How well did your model perform? What types of errors did it make?

```{r}
# get accuracy, ROC AUC, sensitivity, and specificity
accuracy(test_predict, truth = angaus, estimate = .pred_class)
roc_auc(test_predict, truth = angaus, .pred_0)
sensitivity(test_predict, truth = angaus, estimate = .pred_class)
specificity(test_predict, truth = angaus, estimate = .pred_class)
```


[When applied to the testing data, the accuracy of the model was 81%. However, the model made a lot of type I (false-negative) errors, with a specificity of only 48%. This means that when there were in fact eels at the site, the model only correctly predicted that this was the case about 48% of the time. Meanwhile, the sensitivity of the model is high at 89% (i.e., when there were no eels at the site, the model correctly predicted that this was the case 89% of the time).]{style="color:navy;"}

## Fit your model the evaluation data and compare performance

```{r}
# load evaluation data
eval_data <- read.csv(here::here("lab", "data", "eel.eval.data.csv")) %>%
  janitor::clean_names() %>%
  mutate(angaus = as.factor(angaus_obs),
         ds_dam = as.factor(ds_dam)) %>% 
  select(-angaus_obs, angaus, everything())

# bake training data using recipe
baked_eval <- bake(eel_recipe, eval_data)

# fit final model to training data
final_fit <- fit(final_xgboost_model, formula = angaus ~ ., data = baked_eval)
```


1.  Now used your final model to predict on the other dataset (eval.data.csv)

```{r}

# predict evaluation data
eval_predict <- predict(final_fit, baked_eval) %>% 
  bind_cols(baked_eval) # bind predictions column to testing data

# get prediction probabilities
eval_predict <- predict(final_fit, baked_eval, type = "prob") %>%
  bind_cols(eval_predict) # bind predictions column to class predictions data frame

# create confusion matrix
conf_mat(data = eval_predict, truth = angaus, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

2.  How does your model perform on this data?

```{r}
# get accuracy, ROC AUC, sensitivity, and specificity
accuracy(eval_predict, truth = angaus, estimate = .pred_class)
roc_auc(eval_predict, truth = angaus, .pred_0)
sensitivity(eval_predict, truth = angaus, estimate = .pred_class)
specificity(eval_predict, truth = angaus, estimate = .pred_class)
```

[When applied to the evaluation data, the accuracy of the model was 82%, which is about the same as when the model was applied to the testing data. However, the specificity of the model decreases substantially to only 27% (i.e., when there were eels at the site, the model correctly predicted that this was the case 27% of the time), meaning that the model is committing even more type I errors than it was before. Meanwhile, the sensitivity of the model is extremely high at 96% (i.e., when there were no eels at the site, the model correctly predicted that this was the case 96% of the time).]{style="color:navy;"}


3.  How do your results compare to those of Elith et al.?

[My predictive performance results are pretty similar to those of Elith et al. When applied to data that the model hasn't seen before, we both find that the area under the receiver operating characteristic (AUC ROC) to be around 0.85. Specifically, the AUC ROC was 0.83 when I applied my model to my testing data and 0.85 when applied to the evaluation data. Comparatively, Elith et al got an AUC ROC of 0.86 when they applied their model to independent sites.]{style="color:navy;"}

-   Use {vip} to compare variable importance

```{r}
# compare importance of different predictor variables
vip(final_fit, method = "model")
```

[My variable importance results are similar to Elith et al in that summer air temperature (seg_sum_t) was by far the most important predictor variable, though its relative contribution percentage is about double in my analysis. In addition, for both of us, the second most important predictor variable was the proportion of area with indigenous forest (us_native), though its relative contribution percentage is also about double in my analysis. Meanwhile, the most significant difference is the relative contribution of maximum downstream slope (ds_max_slope) in my analysis, which is well ahead of the weighted average of proportional cover of bed sediment (loc_sed). For Elith et al, the weighted average of proportional cover of bed sediment was slightly more important than maximum downstream slope.]{style="color:navy;"}

-   What do your variable importance results tell you about the distribution of this eel species?

[My variable importance results say that summer air temperature is the most significant predictor of eel distribution, explaining about 45% of variation. Furthermore, proportion of area with indigenous forest explains about 21% of the variation in eel distribution, distance to coast explains about 19%, maximum downstream slope explains about 10%, and the weighted average of proportional cover of bed sediment explains about 5%.]{style="color:navy;"}
