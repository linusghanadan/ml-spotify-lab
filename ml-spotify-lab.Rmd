---
title: "Building predictive user classification model with data from Spotify API"
author: "Linus Ghanadan"
date: "2023-02-07"
output: html_document
---

## Background

In this week's lab, we will build models that predict whether a given song is in your collection or in the collection of a partner from class. Specifically, we will compare the performance of models built using a single decision tree, bagged decision trees, a random forest, and stochastic gradient boosting.

## Setup & API access

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```

```{r}
# set seed
set.seed(123)
```

```{r}
# load packages
library(tidyverse)
library(tidymodels)
library(here)
library(patchwork)
library(baguette)
library(vip)
```

```{r, eval=FALSE}
# source API variable
setwd(here::here())
api <- source('keys/keys.R')

# load package for accessing Spotify API
library(spotifyr)

# access API using ID and SECRET
Sys.setenv(SPOTIFY_CLIENT_ID = '78d73a6f06864c0490376e08f8dc5b50')
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'api')

# set an authorization code (will need to retrieve data using R functions)
authorization_code <- get_spotify_authorization_code(scope = scopes()[c(1:19)])

# receive access token from Spotify based on ID and SECRET
access_token <- get_spotify_access_token()
```

## Import & prepare data

You can use get_my_saved_tracks() to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with.

```{r, eval=FALSE}
# for loop to retrieve 150 of my saved tracks
offsets = seq(from = 0, to = 150, by = 50) # set offsets for for loop
my_tracks <- data.frame(matrix(nrow = 0, ncol = 30)) # initialize an empty df 
for (i in seq_along(offsets)) {  
  liked_tracks = get_my_saved_tracks(authorization = authorization_code, limit = 50, 
                                     offset = offsets[i])
  df_temp = as.data.frame(liked_tracks) # create temporary data frame to store the 50 liked tracks from one iteration
  my_tracks <- rbind(my_tracks, df_temp) # bind temporary data frame to my liked tracks data frame 
}
```

These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.

```{r, eval=FALSE}
# get track audio features and bind into one df
first100 <- get_track_audio_features(my_tracks$track.id[1:100])
second100 <- get_track_audio_features(my_tracks$track.id[101:200])
audio_features <- rbind(first100, second100)

# create finalized df of my liked songs with tracks, audio features, and my name
linus_tracks <- my_tracks %>% 
  select(track.name) %>% 
  cbind(audio_features) %>% 
  mutate(name = "linus")

# write CSV file to share with partner
write.csv(linus_tracks,'linus_tracks.csv', row.names = FALSE)
```

Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.

```{r}
# read in my data (CSV that was previously written)
linus_tracks <- read.csv(here::here("data", "linus_tracks.csv"))

# read in partner data
maxwell_tracks <- read.csv(here::here("data", "maxwell_songs.csv")) %>% 
  mutate(name = "maxwell")

# bind my liked songs df with partner df
combined_tracks <- rbind(linus_tracks, maxwell_tracks)
```

## Data exploration

```{r}
# look at summary of columns
summary(combined_tracks)
```


```{r}
# compare mean of certain features for Linus and Maxwell
combined_tracks %>%
  group_by(name) %>%
  summarise(mean_tempo = mean(tempo),
            mean_danceability = mean(danceability),
            mean_valence = mean(valence)) %>% 
  ungroup()
```


```{r}
# compare distribution of certain features for Linus and Maxwell
Hmisc::histbackback(split(combined_tracks$tempo, combined_tracks$name),
             main = "Spotify liked songs comparison of tempo", 
             ylab = "tempo",
             xlab = c("linus", "maxwell"))
Hmisc::histbackback(split(combined_tracks$instrumentalness, combined_tracks$name),
             main = "Spotify liked songs comparison of danceability", 
             ylab = "danceability",
             xlab = c("linus", "maxwell"))
Hmisc::histbackback(split(combined_tracks$valence, combined_tracks$name),
             main = "Spotify liked songs comparison of valence", 
             ylab = "valence",
             xlab = c("linus", "maxwell"))
```

## Data pre-processing

```{r}
# remove irrelevant columns from combined_tracks df
combined_tracks <- combined_tracks %>% 
  select(-track.name, -type, -id, -uri, -track_href, -analysis_url)

# initial split of data into training and testing sets (default 75/25)
tracks_split <- initial_split(combined_tracks)
tracks_test <- testing(tracks_split)
tracks_train <- training(tracks_split)

# specify recipe for model preprocessing
tracks_recipe <- recipe(name ~ ., data = tracks_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() # prep recipe

# create 10 folds of the training data set for CV
cv_folds <- tracks_train %>% vfold_cv(v = 10)
```

## Decision tree model

#### Build preliminary model & tune hyperparameters

```{r}
# specify model for tuning hyperparameters
single_tree_spec <- decision_tree(
  cost_complexity = tune(), # tune cost complexity for pruning tree
  tree_depth = tune(), # tune maximum tree depth
  min_n = tune()) %>% # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)
  set_engine("rpart") %>%
  set_mode("classification")

# create tuning grid for hyperparameters
tuning_grid <- grid_latin_hypercube(cost_complexity(),
                                    tree_depth(),
                                    min_n(),
                                    size = 10)

# create workflow for tuning hyperparameters
single_tree_wf <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(single_tree_spec)

# tune hyperparameters using CV
single_tree_tune <- tune_grid(single_tree_spec, 
                              as.factor(name) ~ ., 
                              resamples = cv_folds,
                              grid = tuning_grid,
                              metrics = metric_set(accuracy))
```

#### Build final model & predict testing data

```{r}
# specify final model with optimized hyperparameters
single_tree_final <- finalize_model(single_tree_spec, select_best(single_tree_tune))

# fit final model to training data
single_tree_fit <- fit(single_tree_final, as.factor(name)~., tracks_train)

# predict testing data
single_tree_predict <- predict(single_tree_fit, tracks_test) %>%
  bind_cols(tracks_test) %>%  # bind to testing df
  mutate(name = as.factor(name))

# get probabilities for predictions made on testing data (to calculate ROC AUC)
single_tree_predict <- predict(single_tree_fit, tracks_test, type = "prob") %>%
  bind_cols(single_tree_predict) %>%  # bind to df that was just created
  mutate(name = as.factor(name))

# store confusion matrix for predictions made on testing data
single_tree_conf_matrix <- single_tree_predict %>% 
  conf_mat(truth = name, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") +
  ggtitle("Single DT") +
  theme_bw() +
  theme(legend.position = "none")

# store error metrics of testing data predictions
single_tree_accuracy <- accuracy(single_tree_predict, truth = name, estimate = .pred_class)
single_tree_roc_auc <- roc_auc(single_tree_predict, truth = name, .pred_linus)
single_tree_sensitivity <- sensitivity(single_tree_predict, truth = name, estimate = .pred_class)
single_tree_specificity <- specificity(single_tree_predict, truth = name, estimate = .pred_class)

```

## Bagged trees model

#### Build final model & predict testing data (no tuning required)

```{r}
# specify model
bagged_trees_spec <- bag_tree() %>%
  set_engine("rpart", times = 50) %>% # specify number of trees (50-500 trees is usually sufficient)
  set_mode("classification")

# create workflow
bagged_trees_wf <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(bagged_trees_spec)

# fit model to training data
bagged_trees_fit <- bagged_trees_wf %>%
  fit(data = tracks_train)

# predict testing data
bagged_trees_predict <- predict(bagged_trees_fit, tracks_test) %>% 
  bind_cols(tracks_test) %>%  # bind to testing df
  mutate(name = as.factor(name))

# get probabilities for predictions made on testing data (to calculate ROC AUC)
bagged_trees_predict <- predict(bagged_trees_fit, tracks_test, type = "prob") %>%
  bind_cols(bagged_trees_predict) %>%  # bind to df that was just created
  mutate(name = as.factor(name))

# store confusion matrix for predictions made on testing data
bagged_trees_conf_matrix <- bagged_trees_predict %>% 
  conf_mat(truth = name, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") +
  ggtitle("Bagged DTs") +
  theme_bw() +
  theme(legend.position = "none")

# store error metrics of testing data predictions
bagged_trees_accuracy <- accuracy(bagged_trees_predict, truth = name, estimate = .pred_class)
bagged_trees_roc_auc <- roc_auc(bagged_trees_predict, truth = name, .pred_linus)
bagged_trees_sensitivity <- sensitivity(bagged_trees_predict, truth = name, estimate = .pred_class)
bagged_trees_specificity <- specificity(bagged_trees_predict, truth = name, estimate = .pred_class)

```

## Random forest model

#### Build preliminary model & tune hyperparameters

```{r}
# specify model for tuning hyperparameters
rf_spec <- rand_forest(trees = 500, # set number of trees to 500
                       mtry = tune(), # tune mtry (number of unique feature variables that will be considered at each split)
                       min_n = tune()) %>% # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)
  set_engine("ranger") %>%
  set_mode("classification")

# create tuning grid for hyperparameters
  tuning_grid <- grid_latin_hypercube(mtry(range = c(2, 4)), 
                                      min_n(c(1, 10)),
                                      size = 10)

# create workflow for tuning hyperparameters
rf_wf <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(rf_spec)

# tune hyperparameters using CV
rf_tune <- tune_grid(rf_wf,
                     resamples = cv_folds,
                     grid = tuning_grid,
                     metrics = metric_set(accuracy))
```

#### Build final model & predict testing data

```{r}
# specify final model with optimized hyperparameters
rf_final <- finalize_model(rf_spec, select_best(rf_tune))

# create workflow for final version of model
rf_final_wf <- workflow() %>%
  add_recipe(tracks_recipe) %>%
  add_model(rf_final)

# fit final workflow to training data
rf_fit <- rf_final_wf %>%
  fit(data = tracks_train)

# predict testing data
rf_predict <- predict(rf_fit, tracks_test) %>%
  bind_cols(tracks_test) %>%  # bind to testing df
  mutate(name = as.factor(name))

# get probabilities for predictions made on testing data (to calculate ROC AUC)
rf_predict <- predict(rf_fit, tracks_test, type = "prob") %>%
  bind_cols(rf_predict) %>%  # bind to df that was just created
  mutate(name = as.factor(name))

# store confusion matrix for predictions made on testing data
rf_conf_matrix <- rf_predict %>% 
  conf_mat(truth = name, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") +
  ggtitle("Random Forest") +
  theme_bw() +
  theme(legend.position = "none")

# store error metrics of testing data predictions
rf_accuracy <- accuracy(rf_predict, truth = name, estimate = .pred_class)
rf_roc_auc <- roc_auc(rf_predict, truth = name, .pred_linus)
rf_sensitivity <- sensitivity(rf_predict, truth = name, estimate = .pred_class)
rf_specificity <- specificity(rf_predict, truth = name, estimate = .pred_class)

```

## Stochastic Gradient Boosting (SGB) model

#### Build preliminary model & tune learning rate

```{r}
# specify model for tuning learning rate
sgb_lr_spec <- boost_tree(mode = "classification",
                      engine = "xgboost",
                      learn_rate = tune())

# create tuning grid for learning rate
tuning_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

# create workflow for tuning learning rate
sgb_lr_wf <- workflow() %>%
  add_model(sgb_lr_spec) %>%
  add_recipe(tracks_recipe)

# tune learning rate using CV
sgb_lr_tune <- tune_grid(sgb_lr_wf,
                         resamples = cv_folds,
                         grid = tuning_grid,
                         metrics = metric_set(accuracy))

# store optimized learning rate
best_lr <- select_best(sgb_lr_tune)
```

#### Build preliminary model & tune tree parameters

```{r}
# specify model for tuning tree parameters
sgb_tree_spec <- boost_tree(learn_rate = best_lr$learn_rate, # use optimized learning rate from previous step
                            trees = 3000, # set number of trees to 3000
                            tree_depth = tune(), # tune maximum tree depth
                            min_n = tune(), # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)
                            loss_reduction = tune(), # tune loss reduction (minimum loss required for further splits)
                            mode = "classification",
                            engine = "xgboost")

# create tuning grid for tree parameters
tuning_grid <- grid_latin_hypercube(tree_depth(),
                                    min_n(),
                                    loss_reduction(),
                                    size = 10)

# create workflow for tuning tree parameters
sgb_tree_wf <- workflow() %>%
  add_model(sgb_tree_spec) %>%
  add_recipe(tracks_recipe)

# tune tree parameters using CV
sgb_tree_tune <- tune_grid(sgb_tree_wf,
                           resamples = cv_folds,
                           grid = tuning_grid,
                           metrics = metric_set(accuracy))

# store optimized tree parameters
best_tree <- select_best(sgb_tree_tune)
```

#### Build preliminary model & tune stochasticity parameters

```{r}
# specify model for tuning stochasticity parameters
sgb_stochastic_spec <- boost_tree(learn_rate = best_lr$learn_rate, # use optimized learning rate
                                  trees = 3000, # set number of trees to 3000
                                  tree_depth = best_tree$tree_depth, # use optimized maximum tree depth
                                  min_n = best_tree$min_n, # use optimized minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)
                                  loss_reduction = best_tree$loss_reduction, # use optimized loss reduction (minimum loss required for further splits)
                                  mtry = tune(), # tune mtry (number of unique feature variables in each subsample)
                                  sample_size = tune(), # tune sample size (amount of randomly selected data exposed to the fitting routine when conducting stochastic gradient descent at each split)
                                  mode = "classification",
                                  engine = "xgboost")

# specify mtry range based on the number of predictors
mtry_final <- finalize(mtry(), tracks_train)

# create tuning grid for stochasticity parameters
tuning_grid <- grid_latin_hypercube(mtry_final,
                                    sample_size = sample_prop(),
                                    size = 10)

# create workflow for tuning stochasticity parameters
sgb_stochastic_wf <- workflow() %>%
  add_model(sgb_stochastic_spec) %>%
  add_recipe(tracks_recipe)

# tune stochasticity parameters using CV
sgb_stochastic_tune <- tune_grid(sgb_stochastic_wf,
                                 resamples = cv_folds,
                                 grid = tuning_grid,
                                 metrics = metric_set(accuracy))

# store optimized stochasticity parameters
best_stochastic <- select_best(sgb_stochastic_tune)
```

#### Build final model & predict testing data

```{r}
# specify final model with optimized parameters
sgb_final <- finalize_model(sgb_stochastic_spec, best_stochastic)

# fit final model to training data
sgb_fit <- fit(sgb_final, as.factor(name)~., tracks_train)

# predict testing data
sgb_predict <- predict(sgb_fit, tracks_test) %>%
  bind_cols(tracks_test) %>%  # bind to testing df
  mutate(name = as.factor(name))

# get probabilities for predictions made on testing data (to calculate ROC AUC)
sgb_predict <- predict(sgb_fit, tracks_test, type = "prob") %>%
  bind_cols(sgb_predict) %>%  # bind to df that was just created
  mutate(name = as.factor(name))

# store confusion matrix for predictions made on testing data
sgb_conf_matrix <- sgb_predict %>% 
  conf_mat(truth = name, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") +
  ggtitle("SGB") +
  theme_bw() +
  theme(legend.position = "none")

# store error metrics of testing data predictions
sgb_accuracy <- accuracy(sgb_predict, truth = name, estimate = .pred_class)
sgb_roc_auc <- roc_auc(sgb_predict, truth = name, .pred_linus)
sgb_sensitivity <- sensitivity(sgb_predict, truth = name, estimate = .pred_class)
sgb_specificity <- specificity(sgb_predict, truth = name, estimate = .pred_class)
```

## Compare models

```{r}
# display confusion matrices of all four models
single_tree_conf_matrix + bagged_trees_conf_matrix + rf_conf_matrix + sgb_conf_matrix +
  plot_layout(nrow = 2, ncol = 2)
```

```{r}
# create tibble of accuracy and ROC AUC for all four models
metrics_tibble <- tibble(
  Method = factor(rep(c("Single DT", "Bagged DTs", "Random Forest", "SGB"), times = 2),
                  levels = c("Single DT", "Bagged DTs", "Random Forest", "SGB")),
  Metric = rep(c("Accuracy", "Area under Receiver Operating Characteristic (ROC) curve"), each = 4),
  Value = c(single_tree_accuracy$.estimate[1], bagged_trees_accuracy$.estimate[1],
            rf_accuracy$.estimate[1], sgb_accuracy$.estimate[1],
            single_tree_roc_auc$.estimate[1], bagged_trees_roc_auc$.estimate[1],
            rf_roc_auc$.estimate[1], sgb_roc_auc$.estimate[1]))

# create bar plot comparing accuracy and ROC AUC across all four models
ggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + 
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_text(aes(label = sprintf("%.2f", Value),
                y = Value + 0.02),
            position = position_dodge(width = 0.9),
            vjust = 0,
            size = 4) +
  theme_minimal() +
  labs(y = "Metric Value", x = "Model", title = "Model Comparison") +
  scale_fill_brewer(palette = "BuPu") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_blank(),
        legend.position = "top",
        legend.title = element_blank())
```

```{r}
# create tibble of accuracy and ROC AUC for all four models
metrics_tibble <- tibble(
  Method = factor(rep(c("Single DT", "Bagged DTs", "Random Forest", "SGB"), times = 2),
                  levels = c("Single DT", "Bagged DTs", "Random Forest", "SGB")),
  Metric = rep(c("Sensitivity\n(Accuracy when truth was Linus)", "Specificity\n(Accuracy when truth was Maxwell)"), each = 4),
  Value = c(single_tree_sensitivity$.estimate[1], bagged_trees_sensitivity$.estimate[1],
            rf_sensitivity$.estimate[1], sgb_sensitivity$.estimate[1],
            single_tree_specificity$.estimate[1], bagged_trees_specificity$.estimate[1],
            rf_specificity$.estimate[1], sgb_specificity$.estimate[1]))


# create bar plot comparing sensitivity and specificity across all four models
ggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + 
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_text(aes(label = sprintf("%.2f", Value),
                y = Value + 0.02),
            position = position_dodge(width = 0.9),
            vjust = 0,
            size = 4) +
  theme_minimal() +
  labs(y = "Metric Value", x = "Model", title = "Model Comparison") +
  scale_fill_brewer(palette = "Greens") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_blank(),
        legend.position = "top",
        legend.title = element_blank(),
        legend.key.height = unit(10, "mm"))
```

```{r}
# compare importance of different predictor variables in best performing model
vip(sgb_fit, method = "model") +
  ggtitle("Ten most important predictor variables in SGB model") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.y = element_text(color = "black", size = 12))
```
